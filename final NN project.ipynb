{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2PMvWK0-3L9p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to Bulgarian Seq2Seq Neural Machine Translation\n",
    "\n",
    "### A Deep Learning Project by Nikolay Nikolov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The following project is an implementation of the Neural Machine Translation (NMT) approach to machine translation from English to Bulgarian. NMT operates on the premise of predicting the likelihoods of a sequence of words and typically model entire sentences. That is the reason why larger datasets of translated sentences result in better models.\n",
    "\n",
    "The way NMM works is by using vector representations for words and internal states. It features a sequence model that predicts one word at a time. Its prediciton is conditioned on the entire source sentence and what was already produced in the target sentence.\n",
    "\n",
    "This model is based on a system of recurrent neural networks (RNN). It contains a first bidirectional RNN, known as an encoder, that encodes a source sentence, and a second RNN, known as a decoder, that predicts the words in the target language. It also uses a special architecture called LSTM (Long Short-Term Memory) that uses several layers in the repeating module in order to solve the long-term dependency problem, in which an RNN struggles to learn or make connections over a large information gap because of difficulty in encoding long inputs into a single vector.\n",
    "\n",
    "The dataset that I am using for this project is a series of several tens of thousands of English sentences, taken from film subtitles, and their respective translations in Bulgarian. The data was provided by OpenSubtitles. Ideally this would not be the optimal data to use for such a machine translation project, as the vast majority of the phrases have not been translated literally, but instead subtitles are generally translated according to the general meaning of a phrase. However, I chose this project quite late and did not have the necessary time to build a quality dataset of literal translations between the two languages.\n",
    "\n",
    "This is also the reason why I will be testing the model with phrases that it has already seen, instead with a free text, because of the relative lack of literal translations, as well as the relatively small size of the dataset (~ 40k phrases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing our data and looking at a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "duwQTNOA4-_a",
    "outputId": "e6da4842-1ed6-46c0-fa32-b8d715720458"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>bulgarian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6615</th>\n",
       "      <td>Do your best.</td>\n",
       "      <td>Успех.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22480</th>\n",
       "      <td>My little starling.</td>\n",
       "      <td>Звездичке моя.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11986</th>\n",
       "      <td>I am doing this for our family.</td>\n",
       "      <td>Правя това за семейството ни.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14404</th>\n",
       "      <td>I see you're going in for jewelery.</td>\n",
       "      <td>Гледам, че обичаш бижутата.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>Again, I am very sorry, is not your father Fel...</td>\n",
       "      <td>Пак силно се извинявам, баща ви е не е ли оня ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14533</th>\n",
       "      <td>I suppose, in a sense, he's the first modern m...</td>\n",
       "      <td>По някакъв начин, той е първият модерен човек.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7663</th>\n",
       "      <td>Everything you need?</td>\n",
       "      <td>Всичко, което ви трябва?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26685</th>\n",
       "      <td>She was in the garden with mr.</td>\n",
       "      <td>Беше в градината с г-н Доукър, сър.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33528</th>\n",
       "      <td>Was it changed?</td>\n",
       "      <td>Че кога са я подменили?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20533</th>\n",
       "      <td>let me show you who you're dealing with</td>\n",
       "      <td>Нека ви покажа с кого си имате работа.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 english                                          bulgarian\n",
       "6615                                       Do your best.                                             Успех.\n",
       "22480                                My little starling.                                     Звездичке моя.\n",
       "11986                    I am doing this for our family.                      Правя това за семейството ни.\n",
       "14404                I see you're going in for jewelery.                        Гледам, че обичаш бижутата.\n",
       "561    Again, I am very sorry, is not your father Fel...  Пак силно се извинявам, баща ви е не е ли оня ...\n",
       "14533  I suppose, in a sense, he's the first modern m...     По някакъв начин, той е първият модерен човек.\n",
       "7663                                Everything you need?                           Всичко, което ви трябва?\n",
       "26685                     She was in the garden with mr.                Беше в градината с г-н Доукър, сър.\n",
       "33528                                    Was it changed?                            Че кога са я подменили?\n",
       "20533            let me show you who you're dealing with             Нека ви покажа с кого си имате работа."
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv('/content/drive/MyDrive/enbg.txt', encoding = 'utf-8', sep = '\\t', header = None)\n",
    "corpus.columns = ['english', 'bulgarian']\n",
    "corpus.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now investigate if there are any null values that should be taken care of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvLKW9lro0ej",
    "outputId": "1f3393f0-5eb3-4782-c040-fc92bd0835d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "english      0\n",
       "bulgarian    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(corpus).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we will apply several operations on our corpus to make it suitable for a neural network. We will remove any duplicate values (if any exist), we'll lowercase all characters, as well as remove apostrophes and other punctuation, digits, as well as any excessive spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KABOShn1o_-n"
   },
   "outputs": [],
   "source": [
    "corpus.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sQuD97HRp9TP"
   },
   "outputs": [],
   "source": [
    "# Lowercase all capital letters\n",
    "corpus['english']=corpus['english'].apply(lambda x: x.lower())\n",
    "corpus['bulgarian']=corpus['bulgarian'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z5uiuTSvqIpG"
   },
   "outputs": [],
   "source": [
    "# Remove apostrophes\n",
    "corpus['english']=corpus['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "corpus['bulgarian']=corpus['bulgarian'].apply(lambda x: re.sub(\"'\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9zGrx_8BrRPW"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "exclude = set(string.punctuation)\n",
    "corpus['english']=corpus['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "corpus['bulgarian']=corpus['bulgarian'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-lrtX01Krcwl"
   },
   "outputs": [],
   "source": [
    "# Remove digits\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "corpus['english']=corpus['english'].apply(lambda x: x.translate(remove_digits))\n",
    "corpus['bulgarian']=corpus['bulgarian'].apply(lambda x: x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any leading and trailing\n",
    "corpus['english']=corpus['english'].apply(lambda x: x.strip())\n",
    "corpus['bulgarian']=corpus['bulgarian'].apply(lambda x: x.strip())\n",
    "corpus['english']=corpus['english'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "corpus['bulgarian']=corpus['bulgarian'].apply(lambda x: re.sub(\" +\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HdkFkG894eXo",
    "outputId": "ba3ddf20-8ce3-4fe7-b160-47d658d99e82"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>bulgarian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40505</th>\n",
       "      <td>your going will not be necessary</td>\n",
       "      <td>няма да е нужно да заминаваш</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39018</th>\n",
       "      <td>you flew over us several times you know very w...</td>\n",
       "      <td>прелетяхте над нас няколко пъти знаете много д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>along the coastal regions an area about the si...</td>\n",
       "      <td>по крайбрежието район с размерите на великобри...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38509</th>\n",
       "      <td>you are kidding</td>\n",
       "      <td>какво шегуваш се</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37635</th>\n",
       "      <td>without my robe i cannot return</td>\n",
       "      <td>не мога да се върна вкъщи без ангелското си об...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30918</th>\n",
       "      <td>there aint enough here for us</td>\n",
       "      <td>тук няма достатъчно дори за нас</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22598</th>\n",
       "      <td>my wife made made this shrine</td>\n",
       "      <td>жена ми тя ли направи това</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36987</th>\n",
       "      <td>whos wasting their time</td>\n",
       "      <td>кой на кого губи времето</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5585</th>\n",
       "      <td>concentrate eunchae</td>\n",
       "      <td>ела на себе си ън че</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13414</th>\n",
       "      <td>i have to go somewhere ill see you later</td>\n",
       "      <td>трябва ми чист въздух</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 english                                          bulgarian\n",
       "40505                   your going will not be necessary                       няма да е нужно да заминаваш\n",
       "39018  you flew over us several times you know very w...  прелетяхте над нас няколко пъти знаете много д...\n",
       "1068   along the coastal regions an area about the si...  по крайбрежието район с размерите на великобри...\n",
       "38509                                    you are kidding                                   какво шегуваш се\n",
       "37635                    without my robe i cannot return  не мога да се върна вкъщи без ангелското си об...\n",
       "30918                      there aint enough here for us                    тук няма достатъчно дори за нас\n",
       "22598                      my wife made made this shrine                         жена ми тя ли направи това\n",
       "36987                            whos wasting their time                           кой на кого губи времето\n",
       "5585                                 concentrate eunchae                               ела на себе си ън че\n",
       "13414           i have to go somewhere ill see you later                              трябва ми чист въздух"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a start and an end token in the beginning and end of every target sentence. We need these because of the encoder-decoder structure. The model needs to know which token to treat as the beginning and which ones to treat as the follow-ups, as well as when to finalize the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "thYlw9ar47O0"
   },
   "outputs": [],
   "source": [
    "corpus['bulgarian'] = corpus['bulgarian'].apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code extracts all words in both sets and stores them in sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "E5Bez2fj4_eb"
   },
   "outputs": [],
   "source": [
    "all_english_words = set()\n",
    "for english in corpus['english']:\n",
    "    for word in english.split():\n",
    "        if word not in all_english_words:\n",
    "            all_english_words.add(word)\n",
    "\n",
    "all_bulgarian_words = set()\n",
    "for bulgarian in corpus['bulgarian']:\n",
    "    for word in bulgarian.split():\n",
    "        if word not in all_bulgarian_words:\n",
    "            all_bulgarian_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8DL3tp9547w",
    "outputId": "b727dff3-312a-417e-b76b-cbe1f0441ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16654\n",
      "28123\n"
     ]
    }
   ],
   "source": [
    "print(len(all_english_words))\n",
    "print(len(all_bulgarian_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very interesting that the Bulgarian translations contain nearly twice as much unique words as the English phrases. This is likely because of the fact that Bulgarian has much more conjugation forms of verbs than English has, as well as definite articles that are attached to nouns, unlike English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sorting purposes, let's add two more columns that'll indicate the lenght in words of each English and Bulgarian sentence. Counted in the Bulgarian column are the start and end tokens of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kzijF6UG56EF"
   },
   "outputs": [],
   "source": [
    "corpus['english_sentence_length'] = corpus['english'].apply(lambda x:len(x.split(\" \")))\n",
    "corpus['bulgarian_sentence_length'] = corpus['bulgarian'].apply(lambda x:len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "husAgZmBG5r6",
    "outputId": "62a72bdc-9a25-483b-d071-1a7e02aa86bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>bulgarian</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>bulgarian_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a yearold girl needs her mother matt</td>\n",
       "      <td>START_ едно годишно момиче се нуждае от майка ...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a baby</td>\n",
       "      <td>START_ това е бебе _END</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a balloon like that</td>\n",
       "      <td>START_ точно такова балонче _END</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a bandit</td>\n",
       "      <td>START_ бандит _END</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a bankrupt coal mine</td>\n",
       "      <td>START_ фалирала _END</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41176</th>\n",
       "      <td>zoya emilievna you were amazing</td>\n",
       "      <td>START_ зоя прекрасно _END</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41177</th>\n",
       "      <td>zoya its mayakovsky</td>\n",
       "      <td>START_ маяковски зоя _END</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41178</th>\n",
       "      <td>zoya</td>\n",
       "      <td>START_ зоя _END</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41179</th>\n",
       "      <td>zoya yes</td>\n",
       "      <td>START_ зоя _END</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41180</th>\n",
       "      <td>zoya nice to meet you zoya</td>\n",
       "      <td>START_ зоя много ми е приятно _END</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41181 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english  ... bulgarian_sentence_length\n",
       "0      a yearold girl needs her mother matt  ...                        11\n",
       "1                                    a baby  ...                         5\n",
       "2                       a balloon like that  ...                         5\n",
       "3                                  a bandit  ...                         3\n",
       "4                      a bankrupt coal mine  ...                         3\n",
       "...                                     ...  ...                       ...\n",
       "41176       zoya emilievna you were amazing  ...                         4\n",
       "41177                   zoya its mayakovsky  ...                         4\n",
       "41178                                  zoya  ...                         3\n",
       "41179                              zoya yes  ...                         3\n",
       "41180            zoya nice to meet you zoya  ...                         7\n",
       "\n",
       "[41181 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there can theoretically be sentences with any lenght in the dataset, but for the sake of optimization, let's set a threshold for the sentence lenght we'll use for our model. We want to keep only the sentences with lenght equal to or smaller than 20 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aI2-OtlqHFkY"
   },
   "outputs": [],
   "source": [
    "corpus = corpus[corpus['english_sentence_length']<=20]\n",
    "corpus = corpus[corpus['bulgarian_sentence_length']<=20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final form of the corpus that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Rlqdsy9mMNmj",
    "outputId": "c67e16b3-1fd5-4f66-cea6-9102b9f2f408"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>bulgarian</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>bulgarian_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a yearold girl needs her mother matt</td>\n",
       "      <td>START_ едно годишно момиче се нуждае от майка ...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a baby</td>\n",
       "      <td>START_ това е бебе _END</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a balloon like that</td>\n",
       "      <td>START_ точно такова балонче _END</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a bandit</td>\n",
       "      <td>START_ бандит _END</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a bankrupt coal mine</td>\n",
       "      <td>START_ фалирала _END</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41176</th>\n",
       "      <td>zoya emilievna you were amazing</td>\n",
       "      <td>START_ зоя прекрасно _END</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41177</th>\n",
       "      <td>zoya its mayakovsky</td>\n",
       "      <td>START_ маяковски зоя _END</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41178</th>\n",
       "      <td>zoya</td>\n",
       "      <td>START_ зоя _END</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41179</th>\n",
       "      <td>zoya yes</td>\n",
       "      <td>START_ зоя _END</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41180</th>\n",
       "      <td>zoya nice to meet you zoya</td>\n",
       "      <td>START_ зоя много ми е приятно _END</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39654 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english  ... bulgarian_sentence_length\n",
       "0      a yearold girl needs her mother matt  ...                        11\n",
       "1                                    a baby  ...                         5\n",
       "2                       a balloon like that  ...                         5\n",
       "3                                  a bandit  ...                         3\n",
       "4                      a bankrupt coal mine  ...                         3\n",
       "...                                     ...  ...                       ...\n",
       "41176       zoya emilievna you were amazing  ...                         4\n",
       "41177                   zoya its mayakovsky  ...                         4\n",
       "41178                                  zoya  ...                         3\n",
       "41179                              zoya yes  ...                         3\n",
       "41180            zoya nice to meet you zoya  ...                         7\n",
       "\n",
       "[39654 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to store the maximum values of the lenghts of sentences in both languages for array generation later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wJdANOM5PnYk"
   },
   "outputs": [],
   "source": [
    "max_length_src = max(corpus['bulgarian_sentence_length'])\n",
    "max_length_tar = max(corpus['english_sentence_length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few variables we will store the input words, the target words, and the number of encoder and decoder tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65SJuYAWMOcc",
    "outputId": "7ebd5e6a-a029-4c19-e8f1-c666f32befb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16654, 28123)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_english_words))\n",
    "target_words = sorted(list(all_bulgarian_words)) \n",
    "num_encoder_tokens = len(all_english_words)\n",
    "num_decoder_tokens = len(all_bulgarian_words)\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to increase the values in the number of encoder and decoder tokens by one, for zero-padding purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "HNmpfzpXMq8b"
   },
   "outputs": [],
   "source": [
    "num_encoder_tokens += 1\n",
    "num_decoder_tokens += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a dictionary featuring the index of tokens in both languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bBhsqbQPNGtt"
   },
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as create a reverse-lookup token index to decode sentences into something readable at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "txy8hSm3NQVz"
   },
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Mhw3XUttNzC_",
    "outputId": "41a735d0-3c7f-4934-a597-0e3b4ae0cc94"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>bulgarian</th>\n",
       "      <th>english_sentence_length</th>\n",
       "      <th>bulgarian_sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31373</th>\n",
       "      <td>these knickknacks tsars shoulder cord</td>\n",
       "      <td>START_ тези дрънкулки царски акселбанти _END</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19439</th>\n",
       "      <td>its very interesting to hear you talk about so...</td>\n",
       "      <td>START_ много интересно ти да говориш че някой ...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33713</th>\n",
       "      <td>we care about others frank</td>\n",
       "      <td>START_ не ни пука за другите франк _END</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14483</th>\n",
       "      <td>i spoke to her</td>\n",
       "      <td>START_ говорих с нея _END</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26496</th>\n",
       "      <td>she didnt shoot him</td>\n",
       "      <td>START_ не го е застреляла тя _END</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>about this wedding what am i supposed to report</td>\n",
       "      <td>START_ какво да докладвам за тази сватба _END</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32423</th>\n",
       "      <td>this style wouldnt work on anyone but yoon right</td>\n",
       "      <td>START_ само юне може да носи такива неща права...</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23152</th>\n",
       "      <td>no no not at all</td>\n",
       "      <td>START_ не разбира се нищо _END</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9792</th>\n",
       "      <td>he kept on seeing that see lsnt that funny</td>\n",
       "      <td>START_ а той продължи да я вижда във въображен...</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>all of you</td>\n",
       "      <td>START_ всички _END</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39654 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 english  ... bulgarian_sentence_length\n",
       "31373              these knickknacks tsars shoulder cord  ...                         6\n",
       "19439  its very interesting to hear you talk about so...  ...                        13\n",
       "33713                         we care about others frank  ...                         8\n",
       "14483                                     i spoke to her  ...                         5\n",
       "26496                                she didnt shoot him  ...                         7\n",
       "...                                                  ...  ...                       ...\n",
       "375      about this wedding what am i supposed to report  ...                         8\n",
       "32423   this style wouldnt work on anyone but yoon right  ...                        14\n",
       "23152                                   no no not at all  ...                         6\n",
       "9792          he kept on seeing that see lsnt that funny  ...                        16\n",
       "863                                           all of you  ...                         3\n",
       "\n",
       "[39654 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = shuffle(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now perform a train-test split in a 80:20 ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5hjI2awN2IQ",
    "outputId": "df3bf2d6-12f4-4e29-c1de-4efb3aaa729a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31723,), (7931,))"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = corpus['english'], corpus['bulgarian']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next piece of code generates a batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Cd3VlmfXOINJ"
   },
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input sequence\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input sequence\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "R-4WHaY4O2r8"
   },
   "outputs": [],
   "source": [
    "latent_dim=300 # Number of nodes used as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now set up the encoder and decoder layers and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZviK3PT6O4g0"
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GYyh46KAO_HQ"
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jfyWGsKtPBZ0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wM99kwDfPEF2",
    "outputId": "4f8c9c1f-24a2-4497-ed15-646417f4e41f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    4996500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    8437200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 300), (None, 721200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 300),  721200      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 28124)  8465324     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,341,424\n",
      "Trainable params: 23,341,424\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150 epochs is a pretty good training time for this model in order to produce decent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "YgxkizzTPJG2"
   },
   "outputs": [],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 128\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgFJ41r4PM2h",
    "outputId": "28ea56da-e3f6-44a7-93d0-4f3083cd5755"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "247/247 [==============================] - 78s 288ms/step - loss: 2.3471 - val_loss: 2.1160\n",
      "Epoch 2/150\n",
      "247/247 [==============================] - 69s 278ms/step - loss: 2.0386 - val_loss: 2.0255\n",
      "Epoch 3/150\n",
      "247/247 [==============================] - 70s 285ms/step - loss: 1.9257 - val_loss: 1.9809\n",
      "Epoch 4/150\n",
      "247/247 [==============================] - 68s 277ms/step - loss: 1.8410 - val_loss: 1.9546\n",
      "Epoch 5/150\n",
      "247/247 [==============================] - 70s 282ms/step - loss: 1.7677 - val_loss: 1.9309\n",
      "Epoch 6/150\n",
      "247/247 [==============================] - 69s 278ms/step - loss: 1.6999 - val_loss: 1.9064\n",
      "Epoch 7/150\n",
      "247/247 [==============================] - 68s 277ms/step - loss: 1.6356 - val_loss: 1.8883\n",
      "Epoch 8/150\n",
      "247/247 [==============================] - 69s 278ms/step - loss: 1.5771 - val_loss: 1.8806\n",
      "Epoch 9/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 1.5180 - val_loss: 1.8731\n",
      "Epoch 10/150\n",
      "247/247 [==============================] - 68s 276ms/step - loss: 1.4617 - val_loss: 1.8741\n",
      "Epoch 11/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 1.4135 - val_loss: 1.8744\n",
      "Epoch 12/150\n",
      "247/247 [==============================] - 68s 277ms/step - loss: 1.3662 - val_loss: 1.8790\n",
      "Epoch 13/150\n",
      "247/247 [==============================] - 68s 277ms/step - loss: 1.3189 - val_loss: 1.8850\n",
      "Epoch 14/150\n",
      "247/247 [==============================] - 69s 280ms/step - loss: 1.2743 - val_loss: 1.8962\n",
      "Epoch 15/150\n",
      "247/247 [==============================] - 68s 277ms/step - loss: 1.2322 - val_loss: 1.9019\n",
      "Epoch 16/150\n",
      "247/247 [==============================] - 68s 277ms/step - loss: 1.1925 - val_loss: 1.9104\n",
      "Epoch 17/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 1.1529 - val_loss: 1.9203\n",
      "Epoch 18/150\n",
      "247/247 [==============================] - 69s 278ms/step - loss: 1.1142 - val_loss: 1.9383\n",
      "Epoch 19/150\n",
      "247/247 [==============================] - 68s 276ms/step - loss: 1.0768 - val_loss: 1.9504\n",
      "Epoch 20/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 1.0444 - val_loss: 1.9616\n",
      "Epoch 21/150\n",
      "247/247 [==============================] - 68s 276ms/step - loss: 1.0099 - val_loss: 1.9793\n",
      "Epoch 22/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 0.9768 - val_loss: 2.0014\n",
      "Epoch 23/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.9474 - val_loss: 2.0150\n",
      "Epoch 24/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 0.9173 - val_loss: 2.0266\n",
      "Epoch 25/150\n",
      "247/247 [==============================] - 68s 276ms/step - loss: 0.8876 - val_loss: 2.0470\n",
      "Epoch 26/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 0.8605 - val_loss: 2.0636\n",
      "Epoch 27/150\n",
      "247/247 [==============================] - 68s 273ms/step - loss: 0.8355 - val_loss: 2.0781\n",
      "Epoch 28/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.8078 - val_loss: 2.0961\n",
      "Epoch 29/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.7848 - val_loss: 2.1101\n",
      "Epoch 30/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.7623 - val_loss: 2.1273\n",
      "Epoch 31/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.7386 - val_loss: 2.1450\n",
      "Epoch 32/150\n",
      "247/247 [==============================] - 68s 276ms/step - loss: 0.7174 - val_loss: 2.1580\n",
      "Epoch 33/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.6956 - val_loss: 2.1665\n",
      "Epoch 34/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.6750 - val_loss: 2.1842\n",
      "Epoch 35/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.6553 - val_loss: 2.2000\n",
      "Epoch 36/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.6351 - val_loss: 2.2138\n",
      "Epoch 37/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.6183 - val_loss: 2.2262\n",
      "Epoch 38/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.6009 - val_loss: 2.2359\n",
      "Epoch 39/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.5822 - val_loss: 2.2563\n",
      "Epoch 40/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.5678 - val_loss: 2.2660\n",
      "Epoch 41/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.5520 - val_loss: 2.2754\n",
      "Epoch 42/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.5394 - val_loss: 2.2952\n",
      "Epoch 43/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.5268 - val_loss: 2.2981\n",
      "Epoch 44/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.5133 - val_loss: 2.3139\n",
      "Epoch 45/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.5010 - val_loss: 2.3238\n",
      "Epoch 46/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.4891 - val_loss: 2.3303\n",
      "Epoch 47/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.4776 - val_loss: 2.3417\n",
      "Epoch 48/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.4657 - val_loss: 2.3516\n",
      "Epoch 49/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.4550 - val_loss: 2.3573\n",
      "Epoch 50/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 0.4457 - val_loss: 2.3601\n",
      "Epoch 51/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.4374 - val_loss: 2.3721\n",
      "Epoch 52/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.4324 - val_loss: 2.3733\n",
      "Epoch 53/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.4222 - val_loss: 2.3869\n",
      "Epoch 54/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.4158 - val_loss: 2.3879\n",
      "Epoch 55/150\n",
      "247/247 [==============================] - 68s 277ms/step - loss: 0.4085 - val_loss: 2.3964\n",
      "Epoch 56/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.4041 - val_loss: 2.3987\n",
      "Epoch 57/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.3945 - val_loss: 2.4105\n",
      "Epoch 58/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.3884 - val_loss: 2.4156\n",
      "Epoch 59/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.3804 - val_loss: 2.4185\n",
      "Epoch 60/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.3734 - val_loss: 2.4217\n",
      "Epoch 61/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.3668 - val_loss: 2.4341\n",
      "Epoch 62/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.3632 - val_loss: 2.4393\n",
      "Epoch 63/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.3551 - val_loss: 2.4490\n",
      "Epoch 64/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 0.3514 - val_loss: 2.4567\n",
      "Epoch 65/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 0.3464 - val_loss: 2.4616\n",
      "Epoch 66/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.3411 - val_loss: 2.4695\n",
      "Epoch 67/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.3360 - val_loss: 2.4742\n",
      "Epoch 68/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.3300 - val_loss: 2.4764\n",
      "Epoch 69/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.3264 - val_loss: 2.4797\n",
      "Epoch 70/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.3219 - val_loss: 2.4865\n",
      "Epoch 71/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.3141 - val_loss: 2.4894\n",
      "Epoch 72/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.3121 - val_loss: 2.4992\n",
      "Epoch 73/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.3083 - val_loss: 2.4989\n",
      "Epoch 74/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 0.3066 - val_loss: 2.5086\n",
      "Epoch 75/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.3038 - val_loss: 2.5115\n",
      "Epoch 76/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.3001 - val_loss: 2.5173\n",
      "Epoch 77/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.2966 - val_loss: 2.5188\n",
      "Epoch 78/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.2962 - val_loss: 2.5200\n",
      "Epoch 79/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.2922 - val_loss: 2.5273\n",
      "Epoch 80/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.2900 - val_loss: 2.5292\n",
      "Epoch 81/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2864 - val_loss: 2.5285\n",
      "Epoch 82/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.2861 - val_loss: 2.5314\n",
      "Epoch 83/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.2832 - val_loss: 2.5389\n",
      "Epoch 84/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.2813 - val_loss: 2.5412\n",
      "Epoch 85/150\n",
      "247/247 [==============================] - 66s 269ms/step - loss: 0.2783 - val_loss: 2.5422\n",
      "Epoch 86/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2760 - val_loss: 2.5435\n",
      "Epoch 87/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2740 - val_loss: 2.5472\n",
      "Epoch 88/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.2715 - val_loss: 2.5504\n",
      "Epoch 89/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2683 - val_loss: 2.5521\n",
      "Epoch 90/150\n",
      "247/247 [==============================] - 66s 269ms/step - loss: 0.2666 - val_loss: 2.5578\n",
      "Epoch 91/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2646 - val_loss: 2.5567\n",
      "Epoch 92/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2630 - val_loss: 2.5600\n",
      "Epoch 93/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.2606 - val_loss: 2.5642\n",
      "Epoch 94/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.2587 - val_loss: 2.5698\n",
      "Epoch 95/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.2568 - val_loss: 2.5670\n",
      "Epoch 96/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.2552 - val_loss: 2.5736\n",
      "Epoch 97/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.2524 - val_loss: 2.5746\n",
      "Epoch 98/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.2502 - val_loss: 2.5788\n",
      "Epoch 99/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.2464 - val_loss: 2.5832\n",
      "Epoch 100/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.2442 - val_loss: 2.5854\n",
      "Epoch 101/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2425 - val_loss: 2.5869\n",
      "Epoch 102/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.2406 - val_loss: 2.5962\n",
      "Epoch 103/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2406 - val_loss: 2.5967\n",
      "Epoch 104/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2371 - val_loss: 2.6015\n",
      "Epoch 105/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.2360 - val_loss: 2.6087\n",
      "Epoch 106/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.2330 - val_loss: 2.6100\n",
      "Epoch 107/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.2300 - val_loss: 2.6129\n",
      "Epoch 108/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.2285 - val_loss: 2.6118\n",
      "Epoch 109/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.2273 - val_loss: 2.6174\n",
      "Epoch 110/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.2256 - val_loss: 2.6188\n",
      "Epoch 111/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.2238 - val_loss: 2.6179\n",
      "Epoch 112/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2230 - val_loss: 2.6295\n",
      "Epoch 113/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.2213 - val_loss: 2.6285\n",
      "Epoch 114/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.2190 - val_loss: 2.6300\n",
      "Epoch 115/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.2171 - val_loss: 2.6357\n",
      "Epoch 116/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.2156 - val_loss: 2.6374\n",
      "Epoch 117/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2151 - val_loss: 2.6431\n",
      "Epoch 118/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.2124 - val_loss: 2.6401\n",
      "Epoch 119/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2098 - val_loss: 2.6470\n",
      "Epoch 120/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2075 - val_loss: 2.6487\n",
      "Epoch 121/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.2064 - val_loss: 2.6487\n",
      "Epoch 122/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.2050 - val_loss: 2.6574\n",
      "Epoch 123/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2019 - val_loss: 2.6560\n",
      "Epoch 124/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.2029 - val_loss: 2.6584\n",
      "Epoch 125/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.2009 - val_loss: 2.6640\n",
      "Epoch 126/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.2002 - val_loss: 2.6654\n",
      "Epoch 127/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.1987 - val_loss: 2.6695\n",
      "Epoch 128/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.1967 - val_loss: 2.6657\n",
      "Epoch 129/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.1953 - val_loss: 2.6760\n",
      "Epoch 130/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.1951 - val_loss: 2.6845\n",
      "Epoch 131/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.1925 - val_loss: 2.6852\n",
      "Epoch 132/150\n",
      "247/247 [==============================] - 68s 275ms/step - loss: 0.1911 - val_loss: 2.6879\n",
      "Epoch 133/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.1887 - val_loss: 2.6850\n",
      "Epoch 134/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.1882 - val_loss: 2.6960\n",
      "Epoch 135/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.1882 - val_loss: 2.6968\n",
      "Epoch 136/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.1865 - val_loss: 2.7039\n",
      "Epoch 137/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.1845 - val_loss: 2.6970\n",
      "Epoch 138/150\n",
      "247/247 [==============================] - 68s 277ms/step - loss: 0.1830 - val_loss: 2.7032\n",
      "Epoch 139/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.1817 - val_loss: 2.7056\n",
      "Epoch 140/150\n",
      "247/247 [==============================] - 66s 269ms/step - loss: 0.1784 - val_loss: 2.7077\n",
      "Epoch 141/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.1775 - val_loss: 2.7120\n",
      "Epoch 142/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.1770 - val_loss: 2.7172\n",
      "Epoch 143/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.1768 - val_loss: 2.7164\n",
      "Epoch 144/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.1763 - val_loss: 2.7190\n",
      "Epoch 145/150\n",
      "247/247 [==============================] - 67s 270ms/step - loss: 0.1751 - val_loss: 2.7176\n",
      "Epoch 146/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.1732 - val_loss: 2.7264\n",
      "Epoch 147/150\n",
      "247/247 [==============================] - 67s 271ms/step - loss: 0.1717 - val_loss: 2.7257\n",
      "Epoch 148/150\n",
      "247/247 [==============================] - 67s 273ms/step - loss: 0.1726 - val_loss: 2.7309\n",
      "Epoch 149/150\n",
      "247/247 [==============================] - 68s 274ms/step - loss: 0.1701 - val_loss: 2.7311\n",
      "Epoch 150/150\n",
      "247/247 [==============================] - 67s 272ms/step - loss: 0.1699 - val_loss: 2.7312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0cf01e32b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs = epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fOKq8HoDrLHk"
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "WbTpEcJMrR-p"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "OULkcFv0rU1Y"
   },
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_91ONm937ofF",
    "outputId": "bcab530a-01f2-4ca7-8851-cd6d016f6947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: now look what happened\n",
      "Actual Bulgarian Translation:  виж какво стана \n",
      "Predicted Bulgarian Translation:  виж какво стана \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Bulgarian Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Bulgarian Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first translation attempt matches exactly the actual translation that was provided with the dataset.\n",
    "\n",
    "### Attempt 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SP_2Jtd97tHU",
    "outputId": "94b4f523-ae3f-41a7-af4a-bcbfab555874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: could you leave us for a moment\n",
      "Actual Bulgarian Translation:  ще ни оставите ли сами за малко \n",
      "Predicted Bulgarian Translation:  ще ни оставите ли за малко \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Bulgarian Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Bulgarian Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second translation attempt is actually closer to the original sentence than to the provided dataset translation. The dataset translation includes the word \"alone\", which is not present in the English sentence and has correctly not been predicted in the model's translation attempt.\n",
    "\n",
    "### Attempt 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DRLkRKqO7tR9",
    "outputId": "87881179-dd6e-4fe6-fb54-743033e3b258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: hello\n",
      "Actual Bulgarian Translation:  здравейте \n",
      "Predicted Bulgarian Translation:  хей ти \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Bulgarian Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Bulgarian Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third translation attempt is considered valid because even though it does not directly translate the word \"hello\", what it translates is also a greeting, albeit much less formal (\"hey you\"). We can deduce that the model has done a decent job at least in capturing the semantics of the input.\n",
    "\n",
    "### Attempt 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6NeOI4H7top",
    "outputId": "c1410ed5-a884-4129-aa86-2c23f2940066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: you just dont understand that\n",
      "Actual Bulgarian Translation:  ти просто не го разбираш \n",
      "Predicted Bulgarian Translation:  ти просто разбираш ли го казваш \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Bulgarian Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Bulgarian Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this attempt, the model has caught the meaning of the first part of the phrase, as well as of the verb, but has not grasped the negation. It has also added a supplementary verb that is not found in the original sentence.\n",
    "\n",
    "### Attempt 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7QbtudI9ds1",
    "outputId": "15f49013-41d3-4067-f762-7db83a2e8ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: alice lock that door\n",
      "Actual Bulgarian Translation:  алис заключи вратата \n",
      "Predicted Bulgarian Translation:  алис заключи вратата \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Bulgarian Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Bulgarian Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a translation that matches perfectly the provided dataset translation.\n",
    "\n",
    "### Attempt 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LpuNZRhy9l_q",
    "outputId": "e9329d8c-180e-4a84-9b5b-d18e58b2914a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input English sentence: he will kill you on sight\n",
      "Actual Bulgarian Translation:  ще те убие на мига \n",
      "Predicted Bulgarian Translation:  ще те убие \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Bulgarian Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Bulgarian Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final attempt, the model seems to have succeeded in translating only the first half of the phrase, ignoring the second half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Once again, this model is far from anything that can be used in any productive manner, but it is only for demonstration purposes how easy it is to implement such an RNN to accomplish such a task. If this work is expanded with a much larger dataset and more accurate original-translation pairs, a great model can be built that will even be able to handle free text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lstm_seq2seq. (n.d.). Retrieved February 18, 2021, from https://keras.rstudio.com/articles/examples/lstm_seq2seq.html\n",
    "\n",
    "Kalchbrenner, Nal; Blunsom, Philip (2013). \"Recurrent Continuous Translation Models\". _Proceedings of the Association for Computational Linguistics: 1700–1709_.\n",
    "\n",
    "Sepp Hochreiter; Jürgen Schmidhuber (1997). \"Long short-term memory\". _Neural Computation._ 9 (8): 1735–1780."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
